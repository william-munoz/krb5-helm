{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDFS: hdfs://hadoop-master.hadoop-domain.default-tenant.svc.cluster.local:9000\n",
      "V3IO: v3io://users/admin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "hdfs_fs = 'hdfs://hadoop-master.hadoop-domain.default-tenant.svc.cluster.local:9000'\n",
    "v3io_fs =  os.getenv('V3IO_HOME_URL')\n",
    "\n",
    "print(f\"HDFS: {hdfs_fs}\")\n",
    "print(f\"V3IO: {v3io_fs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KRB5CCNAME: FILE:/User/spark/krb5kdc_ccache\n",
      "HADOOP_CONF_DIR: /User/spark/hadoop/\n",
      "KRB5_CONFIG: /User/spark/hadoop/krb5.conf\n",
      "JVM config: -Dsun.zip.disableMemoryMapping=true -Djava.security.krb5.conf=/User/spark/hadoop/krb5.conf -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug=true\n"
     ]
    }
   ],
   "source": [
    "krb5_cc_name = 'FILE:/User/spark/krb5kdc_ccache'\n",
    "hadoop_conf_dir = '/User/spark/hadoop/'\n",
    "krb5_config_file = '/User/spark/hadoop/krb5.conf'\n",
    "jvm_config_option = f\"-Dsun.zip.disableMemoryMapping=true -Djava.security.krb5.conf={krb5_config_file} -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug=true\"\n",
    "\n",
    "print(f\"KRB5CCNAME: {krb5_cc_name}\")\n",
    "print(f\"HADOOP_CONF_DIR: {hadoop_conf_dir}\")\n",
    "print(f\"KRB5_CONFIG: {krb5_config_file}\")\n",
    "print(f\"JVM config: {jvm_config_option}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KRB5CCNAME'] = krb5_cc_name\n",
    "os.environ['HADOOP_CONF_DIR'] = hadoop_conf_dir\n",
    "os.environ['KRB5_CONFIG'] = krb5_config_file\n",
    "\n",
    "# os.environ['SPARK_SUBMIT_OPTS'] = jvm_config_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket cache: FILE:/User/spark/krb5kdc_ccache\n",
      "Default principal: hdfs/hadoop-master.hadoop-domain.default-tenant.svc.cluster.local@EXAMPLE.COM\n",
      "\n",
      "Valid starting     Expires            Service principal\n",
      "12/22/20 12:04:53  12/23/20 12:04:53  krbtgt/EXAMPLE.COM@EXAMPLE.COM\n"
     ]
    }
   ],
   "source": [
    "!kinit -k -t /User/spark/krb5.keytab hdfs/hadoop-master.hadoop-domain.default-tenant.svc.cluster.local@EXAMPLE.COM\n",
    "!klist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Example\") \\\n",
    "    .config('fs.v3io.impl','io.iguaz.v3io.hcfs.V3IOFileSystem') \\\n",
    "    .config('fs.AbstractFileSystem.v3io.impl','io.iguaz.v3io.hcfs.V3IOAbstractFileSystem') \\\n",
    "    .config('spark.driver.extraJavaOptions', jvm_config_option) \\\n",
    "    .config('spark.executor.extraJavaOptions', jvm_config_option) \\\n",
    "    .config('spark.executorEnv.KRB5_CONFIG',krb5_config_file) \\\n",
    "    .config('spark.executorEnv.KRB5CCNAME', krb5_cc_name) \\\n",
    "    .config('spark.executorEnv.HADOOP_CONF_DIR', hadoop_conf_dir) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# This will enable using the Hadoop native libs (rather than the libs packages with Spark)\n",
    "#    .config('spark.executorEnv.LD_LIBRARY_PATH', '/hadoop/lib/native') \\\n",
    "# Run in local mode (not using remote executors)\n",
    "#    .master('local') \\\n",
    "# These seem to only work in Spark >=3.0\n",
    "#    .config('spark.kerberos.access.hadoopFileSystems', hdfs_fs) \\\n",
    "#    .config('spark.kerberos.renewal.credentials','ccache') \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = os.path.join(hdfs_fs,'data.csv')\n",
    "print(hdfs_path)\n",
    "\n",
    "hdfs_df = spark.read.csv(hdfs_path)\n",
    "hdfs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3io_path = os.path.join(v3io_fs,'examples','demo.csv')\n",
    "print(v3io_path)\n",
    "\n",
    "v3io_df = spark.read.csv(v3io_path)\n",
    "v3io_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_df.write.parquet(f'{hdfs_fs}/output.parquet', mode='overwrite')\n",
    "hdfs_df.write.csv(f'{hdfs_fs}/output.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.environ.get('HADOOP_CONF_DIR'):\n",
    "    os.environ.pop('HADOOP_CONF_DIR')\n",
    "if os.environ.get('KRB5CCNAME'):\n",
    "   os.environ.pop('KRB5CCNAME')\n",
    "if os.environ.get('KRB5_CONFIG'):\n",
    "    os.environ.pop('KRB5_CONFIG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
